%\addcontentsline{toc}{chapter}{Development Process}
\chapter{Design}

%You should concentrate on the more important aspects of the design. It is essential that an overview is presented before going into detail. As well as describing the design adopted it must also explain what other designs were considered and why they were rejected.

%The design should describe what you expected to do, and might also explain areas that you had to revise after some investigation.

%Typically, for an object-oriented design, the discussion will focus on the choice of objects and classes and the allocation of methods to classes. The use made of reusable components should be described and their source referenced. Particularly important decisions concerning data structures usually affect the architecture of a system and so should be described here.

%How much material you include on detailed design and implementation will depend very much on the nature of the project. It should not be padded out. Think about the significant aspects of your system. For example, describe the design of the user interface if it is a critical aspect of your system, or provide detail about methods and data structures that are not trivial. Do not spend time on long lists of trivial items and repetitive descriptions. If in doubt about what is appropriate, speak to your supervisor.
 
%You should also identify any support tools that you used. You should discuss your choice of implementation tools - programming language, compilers, database management system, program development environment, etc.

%Some example sub-sections may be as follows, but the specific sections are for you to define. 

\section{Language Choice}
As previously stated the language I chose for this project was Go.
This section aims to show why It was chosen over the other candidates I considered.

\subsection{Go}
Go, also known as Golang, is a statically typed, compiled, concurrent and garbage collected language heavily inspired by C for the general syntax, Limbo for it's use of Communicating Sequential Processes (CSP)\cite{HOARE-CSP} and Python for its emphasis on readability and an all inclusive standard library.
It is relatively modern, only released to the public in 2009, and was created to solve some of the problems experienced with the languages used at Google, namely C\verb!++!, Python and Java.
I began to learn it, through the Golang Challenge\cite{GOLANG-CHALLENGE},  during my industrial year after experiencing some of these problems with python myself.

The creators of the language are all well respected in the world of computing.
Rob Pike is known for Plan 9 and UTF8. 
Robert Griesemer for the Java hotspot VM
Ken Thompson for Unix, Plan 9 and B, the predecessor to C.
Their combined hatred of the complexity and compile times of C\verb!++! where the catalyst for Go's development.
They also saw it as an opportunity to improve upon the aspects of C that have been most problematic over the years including\cite{GO-DESIGN-EMAIL}.

\begin{itemize*}
	\item Optional braces around if statements (The cause of apples GOTO fail bug\cite{GOTOFAIL}).
	\item Pointer arithmetic.
    \item Unchecked array boundaries.
	\item Confusing side effects of post/pre increment and decrement. Go only allows the post version and only as a standalone expression.
	\item GOTO statements that can jump anywhere at any time.
    \item Overloaded return values used to indicate errors.
\end{itemize*}

The language also avoided style wars as it came with a tool, gofmt, that formats code 'the right way'.
This is just one of the ways in which Go is extremely opinionated.
Others include:
\begin{itemize*}
	\item Unused imports and variables are a compile time error.
    \item Variables names should be generally be short. The longer they are in scope the longer and more descriptive the name should be.
    \item The use of interface composition over polymorphism, generics and inheritance.
    \item One loop construct, the for loop. Use with a break statement to create a while or until loop.
\end{itemize*}

It also comes with a simple built in testing framework that encourages tests right from the start and a documentation generator emphasising that.

XXX
It also doesn't hurt that Go has been found that to be a great language to build components traditionally done with C, including things as complicated as a kernel subsystem\cite{GONET}.

Concurrency after moores law, cores increasing faster than ghz.

While this is the language I decided to go with in the end I did consider others.
The candidate list included C, C\verb!++! and OCaml.

\subsection{C}
C is a fast, low level, general purpose compiled language that is extremely powerful.
Unfortunately this power comes at a cost.
Historically the readability of C code is low which means that maintaining and extending code is hard.
This would not a huge problem if I was only creating the project for the dissertation but I hope to continue development after submission.

I considered it as a candidate as many shells have already been written using it proving it is a suitable choice.
This could be considered both an advantage, the abundance of prior art, and a disadvantage, less room to find innovative solutions.

While I have used some C in the past and have read the seminal book, K\&R C, I would not consider myself experienced, especially with the more modern aspects like compiler directives and function annotations.
This could have led to problems around having to learn parts of such a complex language while developing a large project.
Even though I did not end up using this language I spent lots of time reading the source code for dash, bash and busybox which are all written in C.
If I had used it may have helped in my context switches. 

One thing I was worried about was the minimal standard library that C implementations provide.
I assumed that I would be required to spend valuable development time creating standard functions and data structures.
However during my research I discovered that libraries such as GLib and qLibc are commonly used and provide most, if not all, of the things I am used to in a standard library like Python's.

I was also worried about introducing the classes of bugs common in C programs such as use after free, memory leaks, buffer under and over flows and format string attacks,

Something to note is that I discovered a library, libmill\cite{LIBMILL}, which provides functions and macros for Go like CSP.

\subsection{C++}

As with anything based in C, C\verb!++! may be considered the obvious step forward.
I have never used the language but have read the source code for a few projects written using it.
It has a fairly complete standard library and some of the memory management is abstracted behind constructor's and destructor's.
Despite this it is notoriously difficult and If the project where to use it it may be tied to one architecture and OS unless a lot of extra effort was put in.

\subsection{OCaml}
OCaml is a functional, garbage collected, compiled language.
It is a multi paradigm language that allows users to mix object oriented, imperative and functional styles.

I considered it as it is often said that ML languages are superior to imperative ones at creating compilers and interpretors. 
This is because construction and modification of tree structures, such as the abstract syntax tree required in interpretors, using pattern matching and algebraic data types is very idiomatic in these types of languages.

I chose to consider it over other functional languages as it had recently come to my attention when it was used to create the high profile language Hack at Facebook.
As well as this it has been used for the Haxe language, the MirageOS unikernel and many code analysis tools.
These are all complex systems that show that the performance and the bulitin parsing tools are up to tasks well above the complexity of a simple interpretor.

I however have no experience at all with OCaml and very little with ML languages in general.
XXX


\begin{figure}[hp]
    \centering
    \includegraphics[scale=0.7]{shell-design.png}
    \caption[Shell execution pipeline]{Shows how input is transformed along the way to being executed commands.}
    \label{fig:shell-flowchart}
\end{figure}


\section{High level design}
This section will describe the high level design of the shell and illustrate to a user with no knowledge of the internals of a shell how components interact with each other.

Figure~\ref{fig:shell-flowchart} is a visual guide to this though it should be noted that the flowchart is not totally representative of the system and is just here to show the rough progression of data.

Input is received and passed to the lexer.
The lexer transforms the text into tokens which are passed to the parser.
The parser uses it's knowledge of the language to group these tokens into Nodes.
These nodes are then joined to others in a tree structure, also known as a parse tree or abstract syntax tree(AST).
The tree is then evaluated using a depth first traversal strategy.
Nodes in the tree can force branches to be skipped or evaluated multiple times.
Traversal eventually reaches some form of command which requires execution.
Arguments for commands are then expanded replacing substitution markers where required.
These are passed through and used as the arguments for the command or function that is to be executed.
After execution an exit code is returned which is propagated up the tree to make decisions.


\section{In depth design}
This section goes into detail about each of the components, talking about the choices that where made in the design and any of the other options that where considered.

\subsection{Input}
I chose to focus on the internals of the shell before making the input experience 'nicer'.
Input can be accepted in a variety of ways including reading from a pipe or file or through an interactive session.
After researching the complexities surrounding interactivity I decided it would have to be left out due to the time restrictions.
I plan to add interactivity in the future using the go-readline library\cite{GO-READLINE} and because of this some parts of the system where designed so that it could easily be plugged in.

The input text is important as it is used to construct the lexer which the parser requires to operate.

\subsection{Lexer}
The lexer I chose to write was a coroutine based non-deterministic finite automaton as described by rob pike in one of the presentations that inspired this project\cite{PIKE-LEXING-VIDEO}.
However as I came to learn this was not the ideal choice (See XXX).
There was also the choice of using a lexer generator such as ANTLR, but with Go being such a recent language these tools generally don't have support for it.
The Ragel tool is an exception to this.
In this section I will explain some of the lexing complexities that caused me to decide on a handwritten lexer over a generated one.

The lexer operates on the raw text a rune\footnote{Go has the concept of a rune which is a type alias of an int32 used to hold a unicode character.} at a time.
These runes are turned into a stream of normalized tokens, removing comments and whitespace.
In some cases newlines are also removed, e.g., when we are expecting some other terminating token such as the 'fi' following an 'if' block.
The lexer can also detect some errors that don't require deep knowledge of the language grammar such as unterminated strings and incomplete variable substitutions. 

However the POSIX spec is very lenient in most scenarios and many things that would be considered errors in other languages are ignored or resolved in some way.
An example of this is the variable symbol '\$'.
By itself or when followed by a invalid substitution character it becomes a literal character with no special meaning. This is very difficult to encode using a parsing expression grammar(PEG) or in Extended Backusâ€“Naur Form(EBNF).

The lexer also handles other complex tasks such as command substitution lexing, which requires the creation of a sub parser to evaluate correctly and alias expansion.

Alias expansion means that the first word of any line should be checked against a list of expansions, if it matches the word is replaced by the expanded text and lexing begins again at the start of the line.
However as this example shows this can introduce a common compiler construction problems known as left recursion.
\begin{verbatim}
# Direct left recursion
$ alias foo='foo xyx'
$ foo

# Indirect left recursion
$ alias foo='bar'
$ alias bar='baz'
$ alias baz='foo'
$ foo
\end{verbatim}
Care must be taken to avoid this as it will cause an infinite loop breaking the lexer and code generators are usually stumped by this.
Gosh does not currently support aliases but structures have been put in place so adding support in the future will be possible.

The lexer returns items called Lexemes which contain the token type and any additional information.
Both the lexeme and the token type are commonly referred to as tokens.

\subsection{Parser}
The next step is using the Parser to check the semantic meaning of these Tokens and to use them to create the nodes for an AST.
This is also the point where any remaining syntax errors should be detected.

I chose to write a predictive recursive descent parser(RDP) and some of the reasons why are:
\begin{enumerate*}
	\item It is a simple, logical design and is relatively easy to implement by hand.
    \item It works with the class of grammar that the shell language has (
    LL(1) explained further in XXX)
    \item dash and zsh both use RDP's giving some prior art to draw from as well as proving it is a suitable construct for the task.
\end{enumerate*}

The second option I considered was using a parser generator.
Go includes an implementation of yacc, one of the most famous, in its toolchain.
The Bash shell even uses bison to generate its parser.
However I feared that using one would have taught me more about the tool than parsers in general, making debugging of the generated code more difficult 
Chet Ramey, the maintainer of bash has also stated if he was starting over he would write a recursive descent parser himself\cite{BASH-ARCH} instead of using a generator.
This eliminated a LALR parser as a candidate.

The last option was a packrat parser.
Like recursive descent, this is a top down parser but it supports backtracking and is guaranteed to parse any LL(x) or LR(x) grammar in linear time.
This technology is very interesting but as it is still very new compared to other techniques it had not seen wide adoption by language designers.
This leads to a lack of prior art and other material to study and is the reason I did not consider it as a candidate further.

\subsection{Nodes \& the AST}
The AST returned from the parser can consist of any number of nodes each of which can contain further child trees.
Go allowed me to express nodes in a clean way through the use of interfaces and structs.
This is a big change from how they are expressed in dash's C code.

Nodes in this tree satisfy an interface called Node, which contains a single method called Eval (See code in~\autoref{lst:node-interface}).
Each Node is a separate type that contains fields relevant to itself.
The types then satisfy this interface with code that knows how to perform the actions of that node and hand execution off to child nodes when appropriate.

As with any tree something has to be terminal.
There are three things that are considered terminal when returned from the parser and each is treated differently.

The NodeCommand is terminal since it returns an exit code, but it is also a valid AST.
This node handles execution of system utilities, builtins and user defined functions as well as the redirection of their input and output to the requested files, pipes or file descriptors.
This is known as a 'simple command' in the specification and is the building block for 'compound commands'.

The NodeNoop which is used in a case list, and as the condition for an else statement.
It ignores its arguments, does no work and returns a successful exit code.

The NodeEOF indicate that the input has been exhausted and the shell should return with the last exit code it received.
It is a subtype of the NodeNoop which is achieved by Go's struct embedding feature. (See code in~\autoref{lst:node-noop})
The main loop checks explicitly for it with a type check.
The layout of the parser means that nothing else can receive this as a valid node.

The only design decision here was that nodes had their own eval methods that dispatch further methods.
dash does this through a monolithic function called evaltree which is a big type switch holding all the code required to evaluate any node.
This makes it hard to read

\subsection{Expansion}
While the diagram shows that only NodeCommands undergo expansion this is a slight lie to keep it simple.
The Arg structures contained in a NodeFor and a NodeCase are also expanded when required but they don't cause any command execution.

When evaluation hits a NodeCommand we know we are going to do either set some variables \\ \verb!A=1 B=2!\\ or run some sort of command, with optionally prepended assignments \\ \verb!A=1 foo!.
In either case it is possible that a value exists that requires expansion e.g \\ \verb!A="$X $(foo) $((3+4))" B=*.c!.

The expansion pipeline performs these expansions in order returning a list of strings as some of the expansions can result in multiple words.
These include:
\begin{description*}
	\item Tilde expansion \hfill \\
    	Turning the '\textasciitilde' character into the filepath of home or named directory's.
    	This expansion only occurs at certain positions in unquoted words.
    \item Subshell expansion \hfill \\
    	This expansion starts another evaluation loop with certain IO redirections so the output of the subshell can be placed as a string in its place.
	\item Arithmetic expansion \hfill \\
    	Arithmetic is sufficiently complicated that it requires a call to a separate lexer parser pair.
        This expansion calls the exposed Parse function and receives an integer in return which can be converted to a string.
    \item Variable Substitution \hfill \\
    	There are some complex variable replacements which can alter the contents, assign new values or even exit the shell with an error in some cases.
	\item Filepath globbing \hfill \\
    	Unquoted strings containing globbing meta characters are expanded to a list of files in the current directory that match the pattern.
        If no files match the pattern it is returned as a literal string.
\end{description*}

Bash and other modern shells extend these further to include brace expansion and process substitution.

\subsection{Execution}
Execution is the generic category that encompasses user builtin functions, defined shell functions and calls out to system executables.

If the command contains a slash at any position it is assumed to be a system executable and checks for functions and builtins are skipped.

\subsubsection{Builtin Commands}
Builtin commands are coded in the host language and are part of the gosh codebase.
They also have the ability to modify parts of the shell's execution scope that are not possible to change externally.

There are two types of builtin commands, the special and regular.
Special commands are really extensions to the shells programming language.
They can affect control flow (return, break, continue, etc.), aliases (alias, unalias) and parameters (readonly, local, etc.).
Errors that occur during these commands are fatal to the shell and cause it to exit.
Special commands are first in the order of precedence and posix requires that they cannot be shadowed by user functions however they can still be hidden by aliases since they occur in the lexer.

Regular special commands interact with the shells internal state but errors are not fatal.
These include commands that affect directories (cd, popd), job control (fg, bg), history or manipulate shell attributes.

Other commands can also be builtins for performance reasons.
Commonly implemented internal commands are the echo, true, false and test utilities as these are often used in scripts.
They don't require the overhead of extra interpretation or interaction with system utilities and as such are much faster than an external system call or a shell function.

\subsubsection{User defined functions}
User defined functions assign a name to a compound command that can be used in a simple command.
When a function is invoked the special positional variables (\$0, \$   

User defined functions are created using the shell syntax and like other shell code they are represented internally as an AST.


Functions also have a scope and can set local variables which are removed once the function returned.

\subsubsection{External commands}
a

\section{Arithmetic}
This section is about the arithmetic construct(AC), the reasons why it requires a separate lexer-parser and how this pair differs from the one used for the general shell language.

While the subset of math that the shell understands is much simpler than the shell's language it is still relatively complex. 
To simplify the general parser, when it detects the start of an AC it continues gathering text into a separate buffer for later evaluation.
This is not the only reason it is done this way.
Variables can be assigned and referenced in these equations so they need to be expanded each time it is evaluated.




















\section{Other relevant sections}